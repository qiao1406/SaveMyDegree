import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import scala.io.Source
import scala.collection.mutable.ArrayBuffer
import org.apache.log4j.Logger
import org.apache.log4j.Level


object GenerateGraph {
        
        //def getGraph ( v: RDD[(Long, String)], e: RDD[Edge[Int]] ): Graph = {  return Graph(v, e) }
        
        def getEdgesRDD (sc: SparkContext, fileName: String ): RDD[Edge[Int]] = {
                 /*
                 *  输入文件名根据文件内容建立边的RDD
                 * */
                val e = new ArrayBuffer[Edge[Int]]
                val fe = Source.fromFile(fileName)
                
                for ( line ← fe.getLines() ) {
                        val s = line.replace("\n", "").split(" ")
                        e += Edge(s(0).toLong, s(1).toLong, s(2).toInt)
                }
                
                fe.close()
                println("***边读入完毕***")
                println()
                return sc.makeRDD(e)
                
        }
        
        def getVerticesRDD (sc: SparkContext, fileName: String ): RDD[(Long, IPVerticeNode)] = {
                /*
                 *  输入文件名根据文件内容建立顶点的RDD
                 * */
                
                val v = new ArrayBuffer[(Long, IPVerticeNode)]  // 暂定
                val fv = Source.fromFile(fileName)
                
                for ( line ← fv.getLines() ) {
                        val s = line.replace("\n", "").split(" ")
                        v += new IPVerticeNode(s(0).toLong, s(1).toString())
                        v += Tuple2(s(0).toLong, new IPVerticeNode(s(1).toString()))
                }
                
                fv.close()
                println("***顶点读入完毕***")
                println()
                return sc.makeRDD(v)
                
        }
    
        def main ( args: Array[String] ){
                
                Logger.getLogger("org").setLevel(Level.ERROR)
                
                val conf = new SparkConf().setAppName("Read File")
                val sc = new SparkContext("local",  "test", conf)
                
                //  Arrays to store vertexes and edges
                val vertexArr = new ArrayBuffer[(String)]()
                val edgeArr = new ArrayBuffer[Edge[Int]]()
                
                // read network description file from disk
                 val inputFile = Source.fromFile(raw"/home/qx/SaveMyDegree/network.txt")
                 val lines = inputFile.getLines()
     
                 while (  lines.hasNext  ) {  // deal with one single line
                         
                         val s = lines.next().split(" ")
                         
                         // 避免重复添加顶点
                         if (  vertexArr.exists( { x:String ⇒ x == s(0) } ) == false  )  {
                                 vertexArr += s(0)
                         }
                         if (  vertexArr.exists( { x:String ⇒ x == s(1) } ) == false  )  {
                                 vertexArr += s(1)
                         }
                         
                         // 添加边
                         edgeArr += Edge(  s(0),  s(1),  s(2) )
                         
                 }
                         
                 
                 inputFile.close()
         }
    
    
}